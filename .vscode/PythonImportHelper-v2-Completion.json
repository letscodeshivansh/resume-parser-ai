[
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "pdfplumber",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdfplumber",
        "description": "pdfplumber",
        "detail": "pdfplumber",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "stringify",
        "importPath": "flatted",
        "description": "flatted",
        "isExtraImport": true,
        "detail": "flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "flatted",
        "description": "flatted",
        "isExtraImport": true,
        "detail": "flatted",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612133535",
        "description": ".history.src.main_20240612133535",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nsample_text = \"Experienced software engineer with a background in developing scalable applications.\"\nprint(preprocess_text(sample_text))",
        "detail": ".history.src.main_20240612133535",
        "documentation": {}
    },
    {
        "label": "sample_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612133535",
        "description": ".history.src.main_20240612133535",
        "peekOfCode": "sample_text = \"Experienced software engineer with a background in developing scalable applications.\"\nprint(preprocess_text(sample_text))",
        "detail": ".history.src.main_20240612133535",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612140707",
        "description": ".history.src.main_20240612140707",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612140707",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141210",
        "description": ".history.src.main_20240612141210",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141210",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141210",
        "description": ".history.src.main_20240612141210",
        "peekOfCode": "resume_text = \nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141210",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141222",
        "description": ".history.src.main_20240612141222",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141222",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141222",
        "description": ".history.src.main_20240612141222",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141222",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141224",
        "description": ".history.src.main_20240612141224",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141224",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141224",
        "description": ".history.src.main_20240612141224",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141224",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240612141223",
        "description": ".history.src.process_resume_20240612141223",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.process_resume_20240612141223",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.process_resume_20240612141223",
        "description": ".history.src.process_resume_20240612141223",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240612141223",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240613013448",
        "description": ".history.src.process_resume_20240613013448",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.process_resume_20240613013448",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.process_resume_20240613013448",
        "description": ".history.src.process_resume_20240613013448",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240613013448",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240613013457",
        "description": ".history.src.process_resume_20240613013457",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240613013457",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240614181827",
        "description": ".history.src.process_resume_20240614181827",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240614181827",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614181824",
        "description": ".history.process_resume_20240614181824",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.process_resume_20240614181824",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614214257",
        "description": ".history.process_resume_20240614214257",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.process_resume_20240614214257",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614231005",
        "description": ".history.process_resume_20240614231005",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    print(f'Processing resume text: {resume_text}', file=sys.stderr)  # Log to stderr for visibility\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.process_resume_20240614231005",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615134012",
        "description": ".history.process_resume_20240615134012",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240615134012",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615134012",
        "description": ".history.process_resume_20240615134012",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615134012",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615172623",
        "description": ".history.process_resume_20240615172623",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\n# Function to process the resume text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240615172623",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615172623",
        "description": ".history.process_resume_20240615172623",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615172623",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184007",
        "description": ".history.process_resume_20240615184007",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240615184007",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184010",
        "description": ".history.process_resume_20240615184010",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\n# Function to process the resume text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240615184010",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184010",
        "description": ".history.process_resume_20240615184010",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615184010",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615191746",
        "description": ".history.process_resume_20240615191746",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240615191746",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "def stringify(value):\n    return _stringify(value, separators=(',', ':'))\nassert stringify([None, None]) == '[[null,null]]'\na = []\no = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "a = []\no = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['o']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)\na.append('two')\na.append(True)\no['one'] = 1\no['two'] = 'two'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "b = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)\na.append('two')\na.append(True)\no['one'] = 1\no['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['one']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['one'] = 1\no['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['two']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['three']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])\no['test'] = {'test': 'OK'}",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['a']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])\no['test'] = {'test': 'OK'}\no['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['test']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['test'] = {'test': 'OK'}\no['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['array']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "a2",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "a2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o2",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "str",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "str = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "oo",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "oo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": "process_resume",
        "description": "process_resume",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": "process_resume",
        "documentation": {}
    }
]