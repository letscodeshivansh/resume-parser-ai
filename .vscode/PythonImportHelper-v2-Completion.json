[
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "pdfplumber",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdfplumber",
        "description": "pdfplumber",
        "detail": "pdfplumber",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "FreqDist",
        "importPath": "nltk.probability",
        "description": "nltk.probability",
        "isExtraImport": true,
        "detail": "nltk.probability",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "fuzz",
        "importPath": "fuzzywuzzy",
        "description": "fuzzywuzzy",
        "isExtraImport": true,
        "detail": "fuzzywuzzy",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "FileLinks",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "stringify",
        "importPath": "flatted",
        "description": "flatted",
        "isExtraImport": true,
        "detail": "flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "flatted",
        "description": "flatted",
        "isExtraImport": true,
        "detail": "flatted",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Container",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TextIO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Container",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "pdfminer",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdfminer",
        "description": "pdfminer",
        "detail": "pdfminer",
        "documentation": {}
    },
    {
        "label": "PDFDocument",
        "importPath": "pdfminer.pdfdocument",
        "description": "pdfminer.pdfdocument",
        "isExtraImport": true,
        "detail": "pdfminer.pdfdocument",
        "documentation": {}
    },
    {
        "label": "PDFNoOutlines",
        "importPath": "pdfminer.pdfdocument",
        "description": "pdfminer.pdfdocument",
        "isExtraImport": true,
        "detail": "pdfminer.pdfdocument",
        "documentation": {}
    },
    {
        "label": "PDFXRefFallback",
        "importPath": "pdfminer.pdfdocument",
        "description": "pdfminer.pdfdocument",
        "isExtraImport": true,
        "detail": "pdfminer.pdfdocument",
        "documentation": {}
    },
    {
        "label": "PDFPage",
        "importPath": "pdfminer.pdfpage",
        "description": "pdfminer.pdfpage",
        "isExtraImport": true,
        "detail": "pdfminer.pdfpage",
        "documentation": {}
    },
    {
        "label": "PDFParser",
        "importPath": "pdfminer.pdfparser",
        "description": "pdfminer.pdfparser",
        "isExtraImport": true,
        "detail": "pdfminer.pdfparser",
        "documentation": {}
    },
    {
        "label": "PDFObjectNotFound",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "PDFValueError",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "PDFStream",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "PDFObjRef",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "resolve1",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "stream_value",
        "importPath": "pdfminer.pdftypes",
        "description": "pdfminer.pdftypes",
        "isExtraImport": true,
        "detail": "pdfminer.pdftypes",
        "documentation": {}
    },
    {
        "label": "PSKeyword",
        "importPath": "pdfminer.psparser",
        "description": "pdfminer.psparser",
        "isExtraImport": true,
        "detail": "pdfminer.psparser",
        "documentation": {}
    },
    {
        "label": "PSLiteral",
        "importPath": "pdfminer.psparser",
        "description": "pdfminer.psparser",
        "isExtraImport": true,
        "detail": "pdfminer.psparser",
        "documentation": {}
    },
    {
        "label": "LIT",
        "importPath": "pdfminer.psparser",
        "description": "pdfminer.psparser",
        "isExtraImport": true,
        "detail": "pdfminer.psparser",
        "documentation": {}
    },
    {
        "label": "isnumber",
        "importPath": "pdfminer.utils",
        "description": "pdfminer.utils",
        "isExtraImport": true,
        "detail": "pdfminer.utils",
        "documentation": {}
    },
    {
        "label": "AnyIO",
        "importPath": "pdfminer.utils",
        "description": "pdfminer.utils",
        "isExtraImport": true,
        "detail": "pdfminer.utils",
        "documentation": {}
    },
    {
        "label": "pdfminer.high_level",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdfminer.high_level",
        "description": "pdfminer.high_level",
        "detail": "pdfminer.high_level",
        "documentation": {}
    },
    {
        "label": "LAParams",
        "importPath": "pdfminer.layout",
        "description": "pdfminer.layout",
        "isExtraImport": true,
        "detail": "pdfminer.layout",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612133535",
        "description": ".history.src.main_20240612133535",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nsample_text = \"Experienced software engineer with a background in developing scalable applications.\"\nprint(preprocess_text(sample_text))",
        "detail": ".history.src.main_20240612133535",
        "documentation": {}
    },
    {
        "label": "sample_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612133535",
        "description": ".history.src.main_20240612133535",
        "peekOfCode": "sample_text = \"Experienced software engineer with a background in developing scalable applications.\"\nprint(preprocess_text(sample_text))",
        "detail": ".history.src.main_20240612133535",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612140707",
        "description": ".history.src.main_20240612140707",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612140707",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141210",
        "description": ".history.src.main_20240612141210",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141210",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141210",
        "description": ".history.src.main_20240612141210",
        "peekOfCode": "resume_text = \nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141210",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141222",
        "description": ".history.src.main_20240612141222",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141222",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141222",
        "description": ".history.src.main_20240612141222",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141222",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.main_20240612141224",
        "description": ".history.src.main_20240612141224",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.main_20240612141224",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.main_20240612141224",
        "description": ".history.src.main_20240612141224",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.main_20240612141224",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240612141223",
        "description": ".history.src.process_resume_20240612141223",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.process_resume_20240612141223",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.process_resume_20240612141223",
        "description": ".history.src.process_resume_20240612141223",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240612141223",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240613013448",
        "description": ".history.src.process_resume_20240613013448",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nresume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.src.process_resume_20240613013448",
        "documentation": {}
    },
    {
        "label": "resume_text",
        "kind": 5,
        "importPath": ".history.src.process_resume_20240613013448",
        "description": ".history.src.process_resume_20240613013448",
        "peekOfCode": "resume_text = \"hello i am a react and mern stack developer\"\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240613013448",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240613013457",
        "description": ".history.src.process_resume_20240613013457",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240613013457",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.src.process_resume_20240614181827",
        "description": ".history.src.process_resume_20240614181827",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.src.process_resume_20240614181827",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614181824",
        "description": ".history.process_resume_20240614181824",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.process_resume_20240614181824",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614214257",
        "description": ".history.process_resume_20240614214257",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    processed_tokens = preprocess_text(resume_text)\n    print(json.dumps(processed_tokens))",
        "detail": ".history.process_resume_20240614214257",
        "documentation": {}
    },
    {
        "label": "preprocess_text",
        "kind": 2,
        "importPath": ".history.process_resume_20240614231005",
        "description": ".history.process_resume_20240614231005",
        "peekOfCode": "def preprocess_text(text):\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token.lower() not in stopwords.words('english')]\n    return tokens\nif __name__ == \"__main__\":\n    resume_text = sys.argv[1]\n    print(f'Processing resume text: {resume_text}', file=sys.stderr)  # Log to stderr for visibility\n    processed_tokens = preprocess_text(resume_text)",
        "detail": ".history.process_resume_20240614231005",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615134012",
        "description": ".history.process_resume_20240615134012",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240615134012",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615134012",
        "description": ".history.process_resume_20240615134012",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615134012",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615172623",
        "description": ".history.process_resume_20240615172623",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\n# Function to process the resume text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240615172623",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615172623",
        "description": ".history.process_resume_20240615172623",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615172623",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184007",
        "description": ".history.process_resume_20240615184007",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240615184007",
        "documentation": {}
    },
    {
        "label": "extract_text_from_pdf",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184010",
        "description": ".history.process_resume_20240615184010",
        "peekOfCode": "def extract_text_from_pdf(pdf_path):\n    text = ''\n    with pdfplumber.open(pdf_path) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text()\n    return text\n# Function to process the resume text\ndef process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240615184010",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615184010",
        "description": ".history.process_resume_20240615184010",
        "peekOfCode": "def process_resume(resume_text):\n    # Basic text processing\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    # Frequency distribution of words\n    freq_dist = FreqDist(filtered_text)\n    # Example output: top 10 most common words\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240615184010",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240615191746",
        "description": ".history.process_resume_20240615191746",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240615191746",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155302",
        "description": ".history.process_resume_20240617155302",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nimport pdfplumber\nwith pdfplumber.open('resume.pdf') as pdf:\n    text = ''",
        "detail": ".history.process_resume_20240617155302",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155437",
        "description": ".history.process_resume_20240617155437",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155437",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155458",
        "description": ".history.process_resume_20240617155458",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155458",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155520",
        "description": ".history.process_resume_20240617155520",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155520",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155551",
        "description": ".history.process_resume_20240617155551",
        "peekOfCode": "def process_resume(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resumetext)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155551",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155650",
        "description": ".history.process_resume_20240617155650",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155650",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155738",
        "description": ".history.process_resume_20240617155738",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155738",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617155825",
        "description": ".history.process_resume_20240617155825",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617155825",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160009",
        "description": ".history.process_resume_20240617160009",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160009",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160038",
        "description": ".history.process_resume_20240617160038",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160038",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160040",
        "description": ".history.process_resume_20240617160040",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160040",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160049",
        "description": ".history.process_resume_20240617160049",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160049",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160136",
        "description": ".history.process_resume_20240617160136",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160136",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160138",
        "description": ".history.process_resume_20240617160138",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160138",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160147",
        "description": ".history.process_resume_20240617160147",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160147",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160149",
        "description": ".history.process_resume_20240617160149",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160149",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160228",
        "description": ".history.process_resume_20240617160228",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160228",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160229",
        "description": ".history.process_resume_20240617160229",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160229",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160234",
        "description": ".history.process_resume_20240617160234",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160234",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160236",
        "description": ".history.process_resume_20240617160236",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160236",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160249",
        "description": ".history.process_resume_20240617160249",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160249",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160251",
        "description": ".history.process_resume_20240617160251",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:",
        "detail": ".history.process_resume_20240617160251",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160330",
        "description": ".history.process_resume_20240617160330",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160330",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160340",
        "description": ".history.process_resume_20240617160340",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160340",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160413",
        "description": ".history.process_resume_20240617160413",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160413",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160415",
        "description": ".history.process_resume_20240617160415",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160415",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160421",
        "description": ".history.process_resume_20240617160421",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160421",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160424",
        "description": ".history.process_resume_20240617160424",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160424",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160427",
        "description": ".history.process_resume_20240617160427",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160427",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160428",
        "description": ".history.process_resume_20240617160428",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160428",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160434",
        "description": ".history.process_resume_20240617160434",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160434",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160456",
        "description": ".history.process_resume_20240617160456",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160456",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160502",
        "description": ".history.process_resume_20240617160502",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160502",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160514",
        "description": ".history.process_resume_20240617160514",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160514",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160516",
        "description": ".history.process_resume_20240617160516",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160516",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160521",
        "description": ".history.process_resume_20240617160521",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160521",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "doc = nlp(text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160547",
        "description": ".history.process_resume_20240617160547",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160547",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "tokens = word_tokenize(text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160606",
        "description": ".history.process_resume_20240617160606",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160606",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617160615",
        "description": ".history.process_resume_20240617160615",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617160615",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161252",
        "description": ".history.process_resume_20240617161252",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617161252",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161253",
        "description": ".history.process_resume_20240617161253",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617161253",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161255",
        "description": ".history.process_resume_20240617161255",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617161255",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "def process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161305",
        "description": ".history.process_resume_20240617161305",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\ndef process_resume(resume_text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(resume_text)\n    filtered_text = [w for w in word_tokens if w.lower() not in stop_words and w.isalnum()]\n    freq_dist = FreqDist(filtered_text)\n    common_words = freq_dist.most_common(10)\n    return common_words",
        "detail": ".history.process_resume_20240617161305",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161331",
        "description": ".history.process_resume_20240617161331",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161331",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161731",
        "description": ".history.process_resume_20240617161731",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161731",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.set_frame_on(False)",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161733",
        "description": ".history.process_resume_20240617161733",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161733",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161850",
        "description": ".history.process_resume_20240617161850",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161850",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161852",
        "description": ".history.process_resume_20240617161852",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161852",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.set_frame_on(False)",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161858",
        "description": ".history.process_resume_20240617161858",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161858",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\n# Hide axes\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161905",
        "description": ".history.process_resume_20240617161905",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161905",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161913",
        "description": ".history.process_resume_20240617161913",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)\n    resume_text = sys.argv[1]\n    try:\n        result = process_resume(resume_text)",
        "detail": ".history.process_resume_20240617161913",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nax.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161916",
        "description": ".history.process_resume_20240617161916",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161916",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nax.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161920",
        "description": ".history.process_resume_20240617161920",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161920",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nsns.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161923",
        "description": ".history.process_resume_20240617161923",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nsns.yaxis.set_visible(False)\nax.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161923",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nsns.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161925",
        "description": ".history.process_resume_20240617161925",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(False)\nsns.yaxis.set_visible(False)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161925",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True\nsns.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161936",
        "description": ".history.process_resume_20240617161936",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True\nsns.yaxis.set_visible(False)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161936",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True\nsns.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161938",
        "description": ".history.process_resume_20240617161938",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True\nsns.yaxis.set_visible(False)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161938",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(False)",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161939",
        "description": ".history.process_resume_20240617161939",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(False)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161939",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(Trur)",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161942",
        "description": ".history.process_resume_20240617161942",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(Trur)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161942",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161943",
        "description": ".history.process_resume_20240617161943",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(False)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161943",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161946",
        "description": ".history.process_resume_20240617161946",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161946",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161949",
        "description": ".history.process_resume_20240617161949",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161949",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161955",
        "description": ".history.process_resume_20240617161955",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161955",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617161957",
        "description": ".history.process_resume_20240617161957",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617161957",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, Data Analysis\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162439",
        "description": ".history.process_resume_20240617162439",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617162439",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162506",
        "description": ".history.process_resume_20240617162506",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617162506",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, \"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162511",
        "description": ".history.process_resume_20240617162511",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617162511",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617162527",
        "description": ".history.process_resume_20240617162527",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617162527",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "name = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170839",
        "description": ".history.process_resume_20240617170839",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617170839",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "name = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "phone_number = [token.resume_text for token in doc if token.like_num and len(token.text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170851",
        "description": ".history.process_resume_20240617170851",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617170851",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "name = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "phone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170854",
        "description": ".history.process_resume_20240617170854",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617170854",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndoc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "doc = nlp(resume_text)\nname = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "name",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "name = [ent.resume_text for ent in doc.ents if ent.label_ == 'PERSON']\nphone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "phone_number",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "phone_number = [token.resume_text for token in doc if token.like_num and len(token.resume_text) == 10]\nstop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "stop_words = set(stopwords.words('english'))\ntokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "tokens = word_tokenize(resume_text)\nfiltered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "filtered_tokens",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\njob_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "job_requirements",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "job_requirements = \"Python, Machine Learning, C++, Javascript\"\nskill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "skill_match",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\nmatch_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "match_percentage",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "match_percentage = sum(skill_match) / len(filtered_tokens) * 100\ndata = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "data = {\n    'Name': name[0] if name else 'N/A',\n    'Phone Number': phone_number[0] if phone_number else 'N/A',\n    'Skills': filtered_tokens,\n    'Match Percentage': match_percentage\n}\ndf = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": ".history.process_resume_20240617170915",
        "description": ".history.process_resume_20240617170915",
        "peekOfCode": "df = pd.DataFrame([data])\nsns.barplot(x='Skills', y='Match Percentage', data=df)\nsns.xaxis.set_visible(True)\nsns.yaxis.set_visible(True)\nsns.set_frame_on(True)\nplt.show()\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python process_resume.py <resume_text>\")\n        sys.exit(1)",
        "detail": ".history.process_resume_20240617170915",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617171154",
        "description": ".history.process_resume_20240617171154",
        "peekOfCode": "def process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\n    match_percentage = sum(skill_match) / len(filtered_tokens) * 100 if filtered_tokens else 0",
        "detail": ".history.process_resume_20240617171154",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617171154",
        "description": ".history.process_resume_20240617171154",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndef process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240617171154",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240617171547",
        "description": ".history.process_resume_20240617171547",
        "peekOfCode": "def process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\n    match_percentage = sum(skill_match) / len(filtered_tokens) * 100 if filtered_tokens else 0",
        "detail": ".history.process_resume_20240617171547",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240617171547",
        "description": ".history.process_resume_20240617171547",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndef process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240617171547",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": ".history.process_resume_20240620234529",
        "description": ".history.process_resume_20240620234529",
        "peekOfCode": "def process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\n    match_percentage = sum(skill_match) / len(filtered_tokens) * 100 if filtered_tokens else 0",
        "detail": ".history.process_resume_20240620234529",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240620234529",
        "description": ".history.process_resume_20240620234529",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndef process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": ".history.process_resume_20240620234529",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015721",
        "description": ".history.process_resume_20240623015721",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]",
        "detail": ".history.process_resume_20240623015721",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015724",
        "description": ".history.process_resume_20240623015724",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]",
        "detail": ".history.process_resume_20240623015724",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\nimport pickle\nimport random\n# List all files under the input directory\nfor dirname, _, filenames in os.walk('/kaggle/input'):",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n {'entities': [(1749, 1755, 'Companies worked at'),\n   (1696, 1702, 'Companies worked at'),\n   (1417, 1423, 'Companies worked at'),\n   (1356, 1793, 'Skills'),\n   (1209, 1215, 'Companies worked at'),\n   (1136, 1248, 'Skills'),\n   (928, 932, 'Graduation Year'),",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)\nWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015850",
        "description": ".history.process_resume_20240623015850",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015850",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n {'entities': [(1749, 1755, 'Companies worked at'),\n   (1696, 1702, 'Companies worked at'),\n   (1417, 1423, 'Companies worked at'),\n   (1356, 1793, 'Skills'),\n   (1209, 1215, 'Companies worked at'),\n   (1136, 1248, 'Skills'),\n   (928, 932, 'Graduation Year'),",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)\nWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015901",
        "description": ".history.process_resume_20240623015901",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015901",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n('Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n {'entities': [(1749, 1755, 'Companies worked at'),\n   (1696, 1702, 'Companies worked at'),\n   (1417, 1423, 'Companies worked at'),\n   (1356, 1793, 'Skills'),\n   (1209, 1215, 'Companies worked at'),\n   (1136, 1248, 'Skills'),\n   (928, 932, 'Graduation Year'),",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)\nWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015909",
        "description": ".history.process_resume_20240623015909",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015909",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)\nWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015921",
        "description": ".history.process_resume_20240623015921",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015921",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nNAME                          -Samyuktha Shivakumar\nLOCATION                      -Bangalore\nEMAIL ADDRESS                 -indeed.com/r/Samyuktha-Shivakumar/ cabce09fe942cb85\n!pip install PyMuPDF \nRequirement already satisfied: PyMuPDF in /opt/conda/lib/python3.7/site-packages (1.22.5)\nWARNING: You are using pip version 20.2.4; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015938",
        "description": ".history.process_resume_20240623015938",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015938",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623015950",
        "description": ".history.process_resume_20240623015950",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623015950",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nNAME                          - Alice Clark\nLOCATION                      - Delhi\nDESIGNATION                   - Software Engineer\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft\nCOMPANIES WORKED AT           - Microsoft",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020001",
        "description": ".history.process_resume_20240623020001",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020001",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020012",
        "description": ".history.process_resume_20240623020012",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020012",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nAccuracy: 1.0\nPrecision: 1.0",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020013",
        "description": ".history.process_resume_20240623020013",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020013",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=true_labels, yticklabels=true_labels)",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020039",
        "description": ".history.process_resume_20240623020039",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020039",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=true_labels, yticklabels=true_labels)",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020105",
        "description": ".history.process_resume_20240623020105",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020105",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl','rb'))\ntrain_data[0]\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline from the model\n    if 'ner'not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # adding NER pipeline to nlp model\n        nlp.add_pipe(ner,last=True)\n    #Add labels in the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}-{ent.text}\")\nimport fitz\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=true_labels, yticklabels=true_labels)\nplt.xlabel('Predicted Labels')",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020133",
        "description": ".history.process_resume_20240623020133",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nfrom IPython.display import FileLinks\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020133",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)\n    # Add labels to the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl', 'rb'))\nprint(train_data[0])\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)\n    # Add labels to the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# Trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}- {ent.text}\")\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}- {ent.text}\")\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=true_labels, yticklabels=true_labels)\nplt.xlabel('Predicted Labels')",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": ".history.process_resume_20240623020435",
        "description": ".history.process_resume_20240623020435",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nFileLinks(r'nlp_ner_model')",
        "detail": ".history.process_resume_20240623020435",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "def stringify(value):\n    return _stringify(value, separators=(',', ':'))\nassert stringify([None, None]) == '[[null,null]]'\na = []\no = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "a = []\no = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o = {}\nassert stringify(a) == '[[]]'\nassert stringify(o) == '[{}]'\na.append(a)\no['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['o']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['o'] = o\nassert stringify(a) == '[[\"0\"]]'\nassert stringify(o) == '[{\"o\":\"0\"}]'\nb = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)\na.append('two')\na.append(True)\no['one'] = 1\no['two'] = 'two'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "b = parse(stringify(a))\nassert isinstance(b, list) and b[0] == b\na.append(1)\na.append('two')\na.append(True)\no['one'] = 1\no['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['one']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['one'] = 1\no['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['two']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['two'] = 'two'\no['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['three']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['three'] = True\nassert stringify(a) == '[[\"0\",1,\"1\",true],\"two\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true},\"two\"]'\na.append(o)\no['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])\no['test'] = {'test': 'OK'}",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['a']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['a'] = a\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\"}]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\"},\"two\",[\"2\",1,\"1\",true,\"0\"]]'\na.append({'test': 'OK'})\na.append([1, 2, 3])\no['test'] = {'test': 'OK'}\no['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['test']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['test'] = {'test': 'OK'}\no['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o['array']",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o['array'] = [1, 2, 3]\nassert stringify(a) == '[[\"0\",1,\"1\",true,\"2\",\"3\",\"4\"],\"two\",{\"o\":\"2\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"0\",\"test\":\"3\",\"array\":\"4\"},{\"test\":\"5\"},[1,2,3],\"OK\"]'\nassert stringify(o) == '[{\"o\":\"0\",\"one\":1,\"two\":\"1\",\"three\":true,\"a\":\"2\",\"test\":\"3\",\"array\":\"4\"},\"two\",[\"2\",1,\"1\",true,\"0\",\"3\",\"4\"],{\"test\":\"5\"},[1,2,3],\"OK\"]'\na2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "a2",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "a2 = parse(stringify(a));\no2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "o2",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "o2 = parse(stringify(o));\nassert a2[0] == a2\nassert o2['o'] == o2\nassert a2[1] == 1 and a2[2] == 'two' and a2[3] == True and isinstance(a2[4], dict)\nassert a2[4] == a2[4]['o'] and a2 == a2[4]['o']['a']\nstr = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "str",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "str = parse('[{\"prop\":\"1\",\"a\":\"2\",\"b\":\"3\"},{\"value\":123},[\"4\",\"5\"],{\"e\":\"6\",\"t\":\"7\",\"p\":4},{},{\"b\":\"8\"},\"f\",{\"a\":\"9\"},[\"10\"],\"sup\",{\"a\":1,\"d\":2,\"c\":\"7\",\"z\":\"11\",\"h\":1},{\"g\":2,\"a\":\"7\",\"b\":\"12\",\"f\":6},{\"r\":4,\"u\":\"7\",\"c\":5}]')\nassert str['b']['t']['a'] == 'sup' and str['a'][1]['b'][0]['c'] == str['b']['t']\noo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "oo",
        "kind": 5,
        "importPath": "client.node_modules.flatted.python.test",
        "description": "client.node_modules.flatted.python.test",
        "peekOfCode": "oo = parse('[{\"a\":\"1\",\"b\":\"0\",\"c\":\"2\"},{\"aa\":\"3\"},{\"ca\":\"4\",\"cb\":\"5\",\"cc\":\"6\",\"cd\":\"7\",\"ce\":\"8\",\"cf\":\"9\"},{\"aaa\":\"10\"},{\"caa\":\"4\"},{\"cba\":\"5\"},{\"cca\":\"2\"},{\"cda\":\"4\"},\"value2\",\"value3\",\"value1\"]');\nassert oo['a']['aa']['aaa'] == 'value1' and oo == oo['b'] and oo['c']['ca']['caa'] == oo['c']['ca']\nprint('OK')",
        "detail": "client.node_modules.flatted.python.test",
        "documentation": {}
    },
    {
        "label": "escape",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def escape(s: Union[str, bytes]) -> str:\n    if isinstance(s, bytes):\n        us = str(s, \"latin-1\")\n    else:\n        us = s\n    return ESC_PAT.sub(lambda m: \"&#%d;\" % ord(m.group(0)), us)\ndef dumpxml(out: TextIO, obj: object, codec: Optional[str] = None) -> None:\n    if obj is None:\n        out.write(\"<null />\")\n        return",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "dumpxml",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def dumpxml(out: TextIO, obj: object, codec: Optional[str] = None) -> None:\n    if obj is None:\n        out.write(\"<null />\")\n        return\n    if isinstance(obj, dict):\n        out.write('<dict size=\"%d\">\\n' % len(obj))\n        for (k, v) in obj.items():\n            out.write(\"<key>%s</key>\\n\" % k)\n            out.write(\"<value>\")\n            dumpxml(out, v)",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "dumptrailers",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def dumptrailers(\n    out: TextIO, doc: PDFDocument, show_fallback_xref: bool = False\n) -> None:\n    for xref in doc.xrefs:\n        if not isinstance(xref, PDFXRefFallback) or show_fallback_xref:\n            out.write(\"<trailer>\\n\")\n            dumpxml(out, xref.get_trailer())\n            out.write(\"\\n</trailer>\\n\\n\")\n    no_xrefs = all(isinstance(xref, PDFXRefFallback) for xref in doc.xrefs)\n    if no_xrefs and not show_fallback_xref:",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "dumpallobjs",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def dumpallobjs(\n    out: TextIO,\n    doc: PDFDocument,\n    codec: Optional[str] = None,\n    show_fallback_xref: bool = False,\n) -> None:\n    visited = set()\n    out.write(\"<pdf>\")\n    for xref in doc.xrefs:\n        for objid in xref.get_objids():",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "dumpoutline",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def dumpoutline(\n    outfp: TextIO,\n    fname: str,\n    objids: Any,\n    pagenos: Container[int],\n    password: str = \"\",\n    dumpall: bool = False,\n    codec: Optional[str] = None,\n    extractdir: Optional[str] = None,\n) -> None:",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "extractembedded",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def extractembedded(fname: str, password: str, extractdir: str) -> None:\n    def extract1(objid: int, obj: Dict[str, Any]) -> None:\n        filename = os.path.basename(obj.get(\"UF\") or cast(bytes, obj.get(\"F\")).decode())\n        fileref = obj[\"EF\"].get(\"UF\") or obj[\"EF\"].get(\"F\")\n        fileobj = doc.getobj(fileref.objid)\n        if not isinstance(fileobj, PDFStream):\n            error_msg = (\n                \"unable to process PDF: reference for %r is not a \"\n                \"PDFStream\" % filename\n            )",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "dumppdf",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def dumppdf(\n    outfp: TextIO,\n    fname: str,\n    objids: Iterable[int],\n    pagenos: Container[int],\n    password: str = \"\",\n    dumpall: bool = False,\n    codec: Optional[str] = None,\n    extractdir: Optional[str] = None,\n    show_fallback_xref: bool = False,",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "create_parser",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def create_parser() -> ArgumentParser:\n    parser = ArgumentParser(description=__doc__, add_help=True)\n    parser.add_argument(\n        \"files\",\n        type=str,\n        default=None,\n        nargs=\"+\",\n        help=\"One or more paths to PDF files.\",\n    )\n    parser.add_argument(",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "def main(argv: Optional[List[str]] = None) -> None:\n    parser = create_parser()\n    args = parser.parse_args(args=argv)\n    if args.debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n    if args.outfile == \"-\":\n        outfp = sys.stdout\n    else:\n        outfp = open(args.outfile, \"w\")\n    if args.objects:",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "logger = logging.getLogger(__name__)\nESC_PAT = re.compile(r'[\\000-\\037&<>()\"\\042\\047\\134\\177-\\377]')\ndef escape(s: Union[str, bytes]) -> str:\n    if isinstance(s, bytes):\n        us = str(s, \"latin-1\")\n    else:\n        us = s\n    return ESC_PAT.sub(lambda m: \"&#%d;\" % ord(m.group(0)), us)\ndef dumpxml(out: TextIO, obj: object, codec: Optional[str] = None) -> None:\n    if obj is None:",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "ESC_PAT",
        "kind": 5,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "ESC_PAT = re.compile(r'[\\000-\\037&<>()\"\\042\\047\\134\\177-\\377]')\ndef escape(s: Union[str, bytes]) -> str:\n    if isinstance(s, bytes):\n        us = str(s, \"latin-1\")\n    else:\n        us = s\n    return ESC_PAT.sub(lambda m: \"&#%d;\" % ord(m.group(0)), us)\ndef dumpxml(out: TextIO, obj: object, codec: Optional[str] = None) -> None:\n    if obj is None:\n        out.write(\"<null />\")",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "LITERAL_FILESPEC",
        "kind": 5,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "LITERAL_FILESPEC = LIT(\"Filespec\")\nLITERAL_EMBEDDEDFILE = LIT(\"EmbeddedFile\")\ndef extractembedded(fname: str, password: str, extractdir: str) -> None:\n    def extract1(objid: int, obj: Dict[str, Any]) -> None:\n        filename = os.path.basename(obj.get(\"UF\") or cast(bytes, obj.get(\"F\")).decode())\n        fileref = obj[\"EF\"].get(\"UF\") or obj[\"EF\"].get(\"F\")\n        fileobj = doc.getobj(fileref.objid)\n        if not isinstance(fileobj, PDFStream):\n            error_msg = (\n                \"unable to process PDF: reference for %r is not a \"",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "LITERAL_EMBEDDEDFILE",
        "kind": 5,
        "importPath": "myenv.Scripts.dumppdf",
        "description": "myenv.Scripts.dumppdf",
        "peekOfCode": "LITERAL_EMBEDDEDFILE = LIT(\"EmbeddedFile\")\ndef extractembedded(fname: str, password: str, extractdir: str) -> None:\n    def extract1(objid: int, obj: Dict[str, Any]) -> None:\n        filename = os.path.basename(obj.get(\"UF\") or cast(bytes, obj.get(\"F\")).decode())\n        fileref = obj[\"EF\"].get(\"UF\") or obj[\"EF\"].get(\"F\")\n        fileobj = doc.getobj(fileref.objid)\n        if not isinstance(fileobj, PDFStream):\n            error_msg = (\n                \"unable to process PDF: reference for %r is not a \"\n                \"PDFStream\" % filename",
        "detail": "myenv.Scripts.dumppdf",
        "documentation": {}
    },
    {
        "label": "float_or_disabled",
        "kind": 2,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "def float_or_disabled(x: str) -> Optional[float]:\n    if x.lower().strip() == \"disabled\":\n        return None\n    try:\n        return float(x)\n    except ValueError:\n        raise argparse.ArgumentTypeError(\"invalid float value: {}\".format(x))\ndef extract_text(\n    files: Iterable[str] = [],\n    outfile: str = \"-\",",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "extract_text",
        "kind": 2,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "def extract_text(\n    files: Iterable[str] = [],\n    outfile: str = \"-\",\n    laparams: Optional[LAParams] = None,\n    output_type: str = \"text\",\n    codec: str = \"utf-8\",\n    strip_control: bool = False,\n    maxpages: int = 0,\n    page_numbers: Optional[Container[int]] = None,\n    password: str = \"\",",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "create_parser",
        "kind": 2,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "def create_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(description=__doc__, add_help=True)\n    parser.add_argument(\n        \"files\",\n        type=str,\n        default=None,\n        nargs=\"+\",\n        help=\"One or more paths to PDF files.\",\n    )\n    parser.add_argument(",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "def parse_args(args: Optional[List[str]]) -> argparse.Namespace:\n    parsed_args = create_parser().parse_args(args=args)\n    # Propagate parsed layout parameters to LAParams object\n    if parsed_args.no_laparams:\n        parsed_args.laparams = None\n    else:\n        parsed_args.laparams = LAParams(\n            line_overlap=parsed_args.line_overlap,\n            char_margin=parsed_args.char_margin,\n            line_margin=parsed_args.line_margin,",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "def main(args: Optional[List[str]] = None) -> int:\n    parsed_args = parse_args(args)\n    outfp = extract_text(**vars(parsed_args))\n    outfp.close()\n    return 0\nif __name__ == \"__main__\":\n    sys.exit(main())",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "OUTPUT_TYPES",
        "kind": 5,
        "importPath": "myenv.Scripts.pdf2txt",
        "description": "myenv.Scripts.pdf2txt",
        "peekOfCode": "OUTPUT_TYPES = ((\".htm\", \"html\"), (\".html\", \"html\"), (\".xml\", \"xml\"), (\".tag\", \"tag\"))\ndef float_or_disabled(x: str) -> Optional[float]:\n    if x.lower().strip() == \"disabled\":\n        return None\n    try:\n        return float(x)\n    except ValueError:\n        raise argparse.ArgumentTypeError(\"invalid float value: {}\".format(x))\ndef extract_text(\n    files: Iterable[str] = [],",
        "detail": "myenv.Scripts.pdf2txt",
        "documentation": {}
    },
    {
        "label": "process_resume",
        "kind": 2,
        "importPath": "process_resume",
        "description": "process_resume",
        "peekOfCode": "def process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]\n    match_percentage = sum(skill_match) / len(filtered_tokens) * 100 if filtered_tokens else 0",
        "detail": "process_resume",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": "process_resume",
        "description": "process_resume",
        "peekOfCode": "nlp = spacy.load('en_core_web_sm')\ndef process_resume(resume_text, job_requirements):\n    doc = nlp(resume_text)\n    # Extracting name and phone number\n    name = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n    phone_number = [token.text for token in doc if token.like_num and len(token.text) == 10]\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(resume_text)\n    filtered_tokens = [w for w in tokens if not w.lower() in stop_words and w.isalnum()]\n    skill_match = [fuzz.token_set_ratio(skill, job_requirements) for skill in filtered_tokens]",
        "detail": "process_resume",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "def train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)\n    # Add labels to the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):\n            ner.add_label(ent[2])",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "extract_predicted_labels_from_entities",
        "kind": 2,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "def extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "train_data",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "train_data = pickle.load(open('/kaggle/input/resume-data-with-annotations/train_data.pkl', 'rb'))\nprint(train_data[0])\n# Load Blank Model\nnlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "nlp",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "nlp = spacy.blank('en')\ndef train_model(train_data):\n    # Remove all pipelines and add NER pipeline to the model\n    if 'ner' not in nlp.pipe_names:\n        ner = nlp.create_pipe('ner')\n        # Adding NER pipeline to nlp model\n        nlp.add_pipe(ner, last=True)\n    # Add labels to the NLP pipeline\n    for _, annotation in train_data:\n        for ent in annotation.get('entities'):",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "nlp_model",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "nlp_model = spacy.load('nlp_ner_model')\n# Trying and seeing the prediction of the model\ndoc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}- {ent.text}\")\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "doc = nlp_model(train_data[0][0])\nfor ent in doc.ents:\n    print(f\"{ent.label_.upper():{30}}- {ent.text}\")\nfile_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "file_path = input(\"Enter the path to the PDF file: \").strip()\nif not file_path:\n    print(\"Please provide a valid file path.\")\nelse:\n    doc = fitz.open(file_path)\n    text = \"\"\n    for page in doc:\n        text += page.get_text()\n    tx = \" \".join(text.split('\\n'))\n    print(tx)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "doc = nlp_model(tx)\nfor ent in doc.ents:\n    print(f'{ent.label_.upper():{30}}- {ent.text}')\n# True labels\ntrue_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "true_labels",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "true_labels = ['COLLEGE NAME', 'COMPANIES WORKED AT', 'NAME', 'SKILLS', 'LOCATION', 'DESIGNATION']\ndef extract_predicted_labels_from_entities(entities):\n    predicted_labels = set()  # Initialize an empty set to store predicted labels\n    # Loop through the entities\n    for ent in entities:\n        predicted_labels.add(ent.label_.upper())  # Add the label to the set\n    return list(predicted_labels)  # Convert set to list\n# Example usage:\npredicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "predicted_labels_list",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "predicted_labels_list = extract_predicted_labels_from_entities(doc.ents)\nprint(predicted_labels_list)\n# Compute confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "conf_matrix",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "conf_matrix = confusion_matrix(true_labels, predicted_labels_list)\n# Compute accuracy\naccuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "accuracy = accuracy_score(true_labels, predicted_labels_list)\n# Compute precision\nprecision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "precision",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "precision = precision_score(true_labels, predicted_labels_list, average='weighted')\n# Compute recall\nrecall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "recall",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "recall = recall_score(true_labels, predicted_labels_list, average='weighted')\n# Compute F1-score\nf1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "f1",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "f1 = f1_score(true_labels, predicted_labels_list, average='weighted')\n# Generate classification report\nreport = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "report",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "report = classification_report(true_labels, predicted_labels_list)\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\nprint(\"Classification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=true_labels, yticklabels=true_labels)\nplt.xlabel('Predicted Labels')",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n# Corresponding metric values\nvalues = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()",
        "detail": "process_resume2",
        "documentation": {}
    },
    {
        "label": "values",
        "kind": 5,
        "importPath": "process_resume2",
        "description": "process_resume2",
        "peekOfCode": "values = [accuracy, precision, recall, f1]\n# Plotting the metrics\nplt.figure(figsize=(8, 5))\nsns.barplot(x=metrics, y=values, palette='viridis')\nplt.ylim(0, 1)\nplt.title('Classification Metrics')\nplt.ylabel('Score')\nplt.show()\nFileLinks(r'nlp_ner_model')",
        "detail": "process_resume2",
        "documentation": {}
    }
]